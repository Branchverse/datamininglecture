{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Versuch Recommender Systeme\n",
    "\n",
    "* Autor: Prof. Dr. Johannes Maucher\n",
    "* Datum: 30.09.2015\n",
    "\n",
    "# Einf√ºhrung\n",
    "## Lernziele:\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "* __√Ñhnlichkeit:__ Verfahren zur Bestimmung der √Ñhnlichkeit zwischen Personen (Kunden) und Elementen (Produkten)\n",
    "* __Empfehlungssysteme__ Collaborative Filtering \n",
    "* __Collaborative Filtering:__ Nutzerbezogener Ansatz und elementbasierter Ansatz\n",
    "\n",
    "S√§mtliche Verfahren und Algorithmen werden in Python implementiert.\n",
    "\n",
    "## Theorie zur Vorbereitung\n",
    "### Recommender Systeme\n",
    "Recommender Systeme werden im E-Commerce eingesetzt um Werbung in Form von kundenspezifischen Empfehlungen zu verteilen. Weitl√§ufig bekannt sind die Amazon-Empfehlungen, die entweder per e-mail geschickt oder nach dem Log-In in der Web-Page angezeigt werden. Diese Empfehlungen werden in Abh√§ngigkeit von den bisher vom jeweiligen Kunden gekauften bzw. bewerteten Produkten erstellt. In diesem Versuch werden die derzeit wohl am weitest verbreiteteten Verfahren f√ºr die Erzeugung kundenspezifischer Empfehlungen vorgestellt, darunter das elementweise Collaborative Filtering, welches z.B. auch von Amazon eingesetzt wird.     \n",
    "\n",
    "Direkt-Marketing Methoden wie die kundenspezifische Erzeugung und Bereitstellung von Werbung erfordern detaillierte Kunden- und Warenkorbanalysen. Kunden mit √§hnlichem Kaufverhalten werden in Kundengruppen zusammengefasst. Die Warenkorbanalyse untersucht u.a. welche Waren bevorzugt im Verbund von der gleichen Person gekauft werden. Damit kann ein H√§ndler Werbung in Form von Empfehlungen individuell und gezielt an seine Kunden richten, abh√§ngig davon welcher Kundengruppe er angeh√∂rt und welche Produkte bevorzugt von dieser Kundengruppe nachgefragt werden. \n",
    "\n",
    "Im ersten Teil der √úbung werden fiktive Daten in einer √ºberschaubaren Menge verwendet. Es handelt sich hier um Filmbewertungen. Anhand dieses Beispiels sollen die notwendigen Methoden und Abl√§ufe implementiert und getestet werden. Diese werden im zweiten Teil der √úbung auf echte Daten angewandt. Hierzu werden √ºber eine Python-API Daten vom Internet-Meta-Radio _last.fm_ integriert. Auf der Basis dieser Daten sollen dann Musikempfehlungen f√ºr last.fm User berechnet werden. \n",
    "\n",
    "Recommender Systeme lassen sich mit\n",
    "\n",
    "* Clustering Verfahren\n",
    "* Suchalgorithmen\n",
    "* Collaborativen Filtering \n",
    " \n",
    "realisieren. Am h√§ufigsten wird hierbei das Collaborative Filtering eingesetzt. F√ºr das Collaborative Filtering wird jeder der $M$ User durch einen $N$-dimensionalen Vektor beschrieben, wobei $N$ die Anzahl der Produkte im Angebot des H√§ndlers ist. Jedes Element im Vektor geh√∂rt zu einem speziellen Produkt. Das Element hat den Wert 1, wenn der User dieses Produkt bereits gekauft hat, sonst 0 (andere Wertbelegungen sind m√∂glich, z.B. wenn Produktbewertungen vorliegen). Alle $M$ Zeilenvektoren k√∂nnen zur _User/Item_ Matrix zusammengefasst werden (siehe Abbildung).\n",
    "\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/UserItemMatrix.png\" style=\"width:500px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das traditionelle **userbasierte Collaborative Filtering (UCF)**, benutzt die √Ñhnlichkeit zwischen Benutzern: Um f√ºr User $U_i$ eine Empfehlung zu erzeugen wird zun√§chst der diesem User √§hnlichste Kunde (oder eine Menge vom √§hnlichsten Kunden) ermittelt. Dann werden $U_i$ die Produkte (Items) empfohlen, welche der √§hnlichste Kunde gekauft hat, $U_i$ selbst jedoch noch nicht. \n",
    "\n",
    "Dieser Ansatz skaliert schlecht im Fall sehr gro√üer *User/Item*-Matrizen. Ausserdem ist er f√ºr User, welche erst wenige Produkte gekauft haben unzuverl√§ssig. Besser eignet sich in diesen F√§llen das **itembasierte Collaborative Filtering (ICF)**. Es wird u.a. von Amazon.com eingesetzt. Diese Variante benutzt die √Ñhnlichkeit zwischen Produkten (Items). Dabei sind Produkte umso √§hnlicher je mehr Kunden diese Produkte gemeinsam gekauft haben. F√ºr die Produkte welche ein Referenzuser $U_i$ bereits gekauft hat, werden die √§hnlichsten Produkte ermittelt. Diese √§hnlichsten Produkte werden $U_i$ empfohlen, wenn er sie nicht schon selbst gekauft hat.\n",
    "\n",
    "Im folgenden Abschnitt werden einige gebr√§uchliche Metriken f√ºr die Berechnung der √Ñhnlichkeit zwischen Benutzern oder Artikeln vorgestellt. F√ºr Collaboratives Filtering wird sehr h√§ufig das Cosinus - √Ñhnlichkeitsma√ü eingesetzt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gebr√§uchliche √Ñhnlichkeitsma√üe\n",
    "\n",
    "Die __euklidische Distanz__ $d_E(\\underline{a},\\underline{b})$ zwischen zwei n-dimensionalen Vektoren $\\underline{a}=(a_1,\\ldots,a_n)$ und $\\underline{b}=(b_1,\\ldots,b_n)$ berechnet sich zu\n",
    "\t$$\n",
    "\td_E(\\underline{a},\\underline{b})=\\sqrt{\\sum_{i=1}^n (a_i-b_i)^2}\n",
    "\t$$\n",
    "Zwei Vektoren k√∂nnen als umso √§hnlicher erachtet werden, je kleiner deren euklidische Distanz ist. \n",
    "Ein auf der euklidischen Metrik basierendes √Ñhnlichkeitsma√ü zwischen zwei Vektoren $\\underline{a}$ und $\\underline{b}$ kann durch \n",
    "$$\n",
    "s_E(\\underline{a},\\underline{b})=\\frac{1}{1+d_E(\\underline{a},\\underline{b})}\n",
    "$$\n",
    "angegeben werden.\n",
    "\n",
    "\n",
    "__Pearson Korrelation__\n",
    "Die √Ñhnlichkeit zwischen zwei Vektoren kann auch durch den Pearson-Korrelationskoeffizient $\\rho_{\\underline{a},\\underline{b}}$ ausgedr√ºckt werden. Er berechnet sich zu\n",
    "$$\n",
    "\\rho_{\\underline{a},\\underline{b}}= \\frac{1}{N}\\cdot \\sum\\limits_{i=1}^{N}\\frac{(a_i-\\overline{a})}{\\sigma_a} \\frac{(b_i-\\overline{b})}{\\sigma_b}\n",
    "$$\n",
    "Dabei bezeichnet $N$ die L√§nge der Vektoren, $\\overline{a}$ den Mittelwert und $\\sigma_a$ die Standardabweichung des Vektors $\\underline{a}$. \n",
    "\n",
    "Der Pearson-Korrelationskoeffizient misst die lineare Abh√§ngigkeit zwischen zwei Vektoren. Der maximale Wert von $+1$ wird erreicht, wenn die durch die beiden Vektoren definierten N Punkte im 2-dimensionalen Raum auf einer ansteigenden Geraden liegen. Der Minimalwert von $-1$ wird erreicht, wenn die Punkte auf einer abfallenden Geraden liegen. Der Betrag des Koeffizienten ist umso kleiner, je st√§rker die Punkte von einer fiktiven Geraden (kann durch lineare Regression berechnet werden) abweichen. Der Koeffizient ist $0$ wenn keine lineare Abh√§ngigkeit zwischen den Vektoren besteht.\n",
    "\n",
    "\n",
    "__Cosinus √Ñhnlichkeitsma√ü__\n",
    "Die √Ñhnlichkeit zwischen zwei Vektoren kann auch durch den Cosinus $\\cos(\\underline{a},\\underline{b})$ ausgedr√ºckt werden. Er berechnet sich zu\n",
    "$$\n",
    "\\cos(\\underline{a},\\underline{b})= \\frac{\\underline{a} \\cdot \\underline{b}}{\\left\\|\\underline{a}\\right\\|\\cdot \\left\\|\\underline{b}\\right\\|}\n",
    "$$\n",
    "wobei im Z√§hler das Skalarprodukt der beiden Vektoren steht und mit $\\left\\|\\underline{x}\\right\\|$ der Betrag des Vektors $\\underline{x}$ bezeichnet wird.\n",
    "\n",
    "Falls die Vektoren $\\underline{a}$ und $\\underline{b}$ mittelwertfrei sind, ist der Cosinus-√Ñhnlichkeitswert gleich dem Pearson-Korrelationswert. In der Dokument- und Textanalyse wird vornehmlich das Cosinus-√Ñhnlichkeitsma√ü verwendet. \n",
    "\n",
    "\n",
    "__Russel Rao √Ñhnlichkeitsma√ü__\n",
    "Die Russel Rao-√Ñhnlichkeit zwischen zwei bin√§ren Vektoren $\\underline{a}$ und $\\underline{b}$ mi√üt das Verh√§ltnis zwischen der Anzahl $\\alpha$ der Stellen in denen beide Vektoren den Wert 1 haben und der L√§nge $n$ der Vektoren. Z.B. ist f√ºr die Vektoren $\\underline{a}=(1,0,1,0,0,1)$ und $\\underline{b}=(0,1,1,1,0,1)$ die Russel-Rao-√Ñhnlichkeit $s_{RR}(\\underline{a},\\underline{b})=2/6=0.333$.\n",
    "\n",
    "__Jaccard √Ñhnlichkeitsma√ü__\n",
    "Die Jaccard-√Ñhnlichkeit zwischen zwei bin√§ren Vektoren $\\underline{a}$ und $\\underline{b}$ mi√üt das Verh√§ltnis zwischen der Anzahl $\\alpha$ der Stellen in denen beide Vektoren den Wert $1$ haben und der Anzahl der Stellen in denen mindestens einer der beiden Vektoren ungleich $0$ ist. Z.B. ist f√ºr die Vektoren $\\underline{a}=(1,0,1,0,0,1)$ und $\\underline{b}=(0,1,1,1,0,1)$ die Jaccard-√Ñhnlichkeit $s_{J}(\\underline{a},\\underline{b})=2/5=0.4$. Die Jaccard Metrik wird in diesem Versuch f√ºr die Bestimmung der √Ñhnlichkeit von *last.fm*-Usern eingesetzt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vor dem Versuch zu kl√§rende Fragen\n",
    "Eine Untermenge der im Folgenden aufgef√ºhrten Fragen wird zu Beginn des Versuchs im Rahmen eines Gruppenkolloqs abgefragt. Auf jede Frage sollte von mindestens einem Gruppenmitglied eine Antwort geliefert werden und jedes Gruppenmitglied muss mindestens eine der gestellten Fragen beantworten k√∂nnen.\n",
    "\n",
    "**Aufgaben:**\n",
    "\n",
    "* Beschreiben Sie das Prinzip des userbasierten Collaborativen Filtering (UCF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Matrix aus User und Produkten wird genutzt, um f√ºr einen User, den √§hnlichsten User zu finden, bzw. √§hnliche User werden in eine Guppe eingeordnet. \n",
    "Der User 1 bekommt dann die Produkte vom √§hnlichsten User vorgeschlagen, die UserA schon gekauft, User 1 aber nicht gekauft hat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Welche Nachteile hat das UCF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es skaliert schlecht bei gro√üen User/Produkt Matrixen und wenn User neu sind und noch nicht viel gekauft haben, kann man die Genauigkeit nicht zuverl√§ssig berechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Worin besteht der Unterschied zwischen UCF und itembasierten Collaborativen Filtering (ICF)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICF vergleicht Produkte und f√ºgt sie in Gruppen zusammen wenn sie sich √§hnlich sind. Produkte sind umso √§hnlicher, umso mehr user sie zusammen gekauft haben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gegeben seien die Vektoren \n",
    "\n",
    "    \\begin{eqnarray*}\n",
    "    \\underline{a} & = & [1,2,3,4,5,6] \\\\\n",
    "    \\underline{b} & = & [3,3,5,6,7,8] \\\\\n",
    "    \\end{eqnarray*}\n",
    "    \n",
    "    Schreiben Sie eine Python Funktion, die den Mittelwert derartiger Vektoren berechnet. Schreiben Sie eine weitere Funktion, die die Varianz berechnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "5.333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "ùëé = [1,2,3,4,5,6]\n",
    "ùëè = [3,3,5,6,7,8]\n",
    "vecA = np.array(a)\n",
    "vecB = np.array(b)\n",
    "def vectorMean(a):\n",
    "    mittelwert = np.mean(a)\n",
    "    print(mittelwert)\n",
    "    \n",
    "vectorMean(vecA)\n",
    "vectorMean(vecB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9166666666666665\n",
      "3.5555555555555554\n"
     ]
    }
   ],
   "source": [
    "def vectorVar(a):\n",
    "    varianz = np.var(a)\n",
    "    print(varianz)\n",
    "vectorVar(vecA)\n",
    "vectorVar(vecB)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wie gro√ü ist die\n",
    "\n",
    "    - Euklidische √Ñhnlichkeit\n",
    "    - Pearson √Ñhnlichkeit\n",
    "    - Cosinus √Ñhnlichkeit\n",
    "    \n",
    "    zwischen den Vektoren $\\underline{a}$ und $\\underline{b}$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.991060084745164\n",
      "Pearson:  0.983343422062855\n",
      "Eucliden Similarity:  0.179128784747792\n"
     ]
    }
   ],
   "source": [
    "cosinus = np.dot(vecA,vecB)/(norm(vecA)*norm(vecB))\n",
    "dist = np.sqrt(np.sum(np.square(vecA-vecB)))\n",
    "euclid = 1/(1+dist)\n",
    "print('Cosine Similarity:', cosinus)\n",
    "print('Pearson: ', np.corrcoef(vecA,vecB)[0,1])\n",
    "print('Eucliden Similarity: ', euclid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In welchen F√§llen sind Cosinus- und Pearson√§hnlichkeit der euklidischen √Ñhnlichkeit vorzuziehen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Cosinusdistanz wird der Winkel zwischen den Vektoren gemessen, w√§hrend bei euklid die absolute Distanz gemessen wird. Ist hilfreich bei zum Beispiel Bewertungen. In 2D macht euklidische Distanz Sinn. Im hochdimensionalen Raum ist das nicht unbedingt der k√ºrzeste Weg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pylast in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (5.1.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from pylast) (0.23.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from httpx->pylast) (1.3.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from httpx->pylast) (0.15.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from httpx->pylast) (1.5.0)\n",
      "Requirement already satisfied: certifi in d:\\unilea\\anaconda\\envs\\datamining\\lib\\site-packages (from httpx->pylast) (2022.9.24)\n",
      "Requirement already satisfied: anyio==3.* in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from httpcore<0.16.0,>=0.15.0->httpx->pylast) (3.6.2)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from httpcore<0.16.0,>=0.15.0->httpx->pylast) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\leagu\\appdata\\roaming\\python\\python310\\site-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx->pylast) (3.4)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pylast'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Latex\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpylast\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pylast'"
     ]
    }
   ],
   "source": [
    "!pip3 install pylast\n",
    "from IPython.display import Latex\n",
    "from IPython.display import Image\n",
    "import pylast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuchsdurchf√ºhrung\n",
    "## Teil 1: Fiktive Filmbewertung\n",
    "### Daten\n",
    "Folgende Tabelle enth√§lt die Filmbewertungen von 7 Personen.\n",
    "from IPython.display import Latex\n",
    "In diesem Versuch sollen Kenntnisse in folgenden Themen vermittelt werden:\n",
    "\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/recommenderFilmRecommendations.PNG\" style=\"width:500px\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Tabelle ist als Python dictionary _critics_ implementiert. Die Keys des Python-Dictionary definieren die Namen von Personen (Zeilen in der Matrix), die Filme bewertet haben. Die Values sind selbst wieder Dictionarys, welche als Keys die Filmnamen (Spalten in der Matrix) und als Values die jeweilige Filmbewertung (Matrixelment) enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n",
    " 'The Night Listener': 3.0},\n",
    "'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n",
    " 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n",
    " 'You, Me and Dupree': 3.5}, \n",
    "'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n",
    " 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n",
    "'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n",
    " 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n",
    " 'You, Me and Dupree': 2.5},\n",
    "'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n",
    " 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 2.0}, \n",
    "'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n",
    "'Toby': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √Ñhnlichkeiten berechnen\n",
    "\n",
    "F√ºr die Bestimmung der √Ñhnlichkeit zwischen Personen und Produkten werden in diesem Versuch ein auf der euklidischen Distanz basierendes √Ñhnlichkeitsma√ü und die Pearson-Korrelation verwendet. Beide √Ñhnlichkeitsma√üe sind in den unten definierten Funktionen implementiert. Alle drei hier implementierten Funktionen zur Berechnung der √Ñhnlichkeit erhalten als √úbergabeparameter das oben definierte Dictionary, das die Filmbewertungen enth√§lt und die Namen der zwei Personen, die verglichen werden sollen. \n",
    "\n",
    "Zu beachten ist, dass in beiden Funktionen f√ºr die Berechnung der √Ñhnlichkeit zwischen zwei Personen nur die Produkte ber√ºcksichtigt werden, welche von beiden Personen schon bewertet wurden. Es handelt sich hier also um modifizierte √Ñhnlichkeitsfunktionen. \n",
    "\n",
    "__Aufgabe:__\n",
    "Fragen Sie von diesem Dictionary _Toby's_ Bewertung des Films _Snakes on a Plane_ ab und geben Sie diesen Wert aus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tobys Bewertung f√ºr 'Snakes on a Plane':  4.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Tobys Bewertung f√ºr 'Snakes on a Plane': \", critics.get(\"Toby\").get(\"Snakes on a Plane\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial.distance as sci\n",
    "\n",
    "\n",
    "def sim_euclid(prefs,person1,person2,normed=True):\n",
    "    ''' Returns a euclidean-distance-based similarity score for \n",
    "    person1 and person2. In the distance calculation the sum is computed \n",
    "    only over those items, which are nonzero for both instances, i.e. only\n",
    "    films which are ranked by both persons are regarded.\n",
    "    If the parameter normed is True, then the euclidean distance is divided by\n",
    "    the number of non-zero elements integrated in the distance calculation. Thus\n",
    "    the effect of larger distances in the case of an increasing number of commonly ranked\n",
    "    items is avoided.\n",
    "    '''\n",
    "    # Get the list of shared_items\n",
    "    si={}\n",
    "    for item in prefs[person1]: \n",
    "        if item in prefs[person2]: \n",
    "            si[item] = 1\n",
    "            \n",
    "    # len(si) counts the number of common ratings\n",
    "    # if they have no ratings in common, return 0\n",
    "    if len(si) == 0: \n",
    "        return 0\n",
    "\n",
    "    # Add up the squares of all the differences\n",
    "    sum_of_squares=np.sqrt(sum([pow(prefs[person1][item]-prefs[person2][item],2) \n",
    "                     for item in prefs[person1] if item in prefs[person2]]))\n",
    "    if normed:\n",
    "        sum_of_squares= 1.0/len(si)*sum_of_squares\n",
    "    return 1/(1+sum_of_squares)\n",
    "\n",
    "\n",
    "def sim_pearson(prefs,p1,p2):\n",
    "    '''\n",
    "    Returns the Pearson correlation coefficient for p1 and p2\n",
    "    '''\n",
    "\n",
    "    # Get the list of commonly rated items\n",
    "    si={}\n",
    "    for item in prefs[p1]: \n",
    "        if item in prefs[p2]: \n",
    "            si[item]=1\n",
    "\n",
    "    # if they are no ratings in common, return 0\n",
    "    if len(si)==0: return 0\n",
    "\n",
    "    # Sum calculations\n",
    "    n=len(si)\n",
    "\n",
    "    # Calculate means of person 1 and 2\n",
    "    mp1=np.mean([prefs[p1][it] for it in si])\n",
    "    mp2=np.mean([prefs[p2][it] for it in si])\n",
    "\n",
    "    # Calculate standard deviation of person 1 and 2\n",
    "    sp1=np.std([prefs[p1][it] for it in si])\n",
    "    sp2=np.std([prefs[p2][it] for it in si])\n",
    "\n",
    "    # If all elements in one sample are identical, the standard deviation is 0. \n",
    "    # In this case there is no linear correlation between the samples\n",
    "    if sp1==0 or sp2==0:\n",
    "        return 0\n",
    "    r=1/(n*sp1*sp2)*sum([(prefs[p1][it]-mp1)*(prefs[p2][it]-mp2) for it in si])\n",
    "    return r\n",
    "\n",
    "\n",
    "def sim_RusselRao(prefs,person1,person2,normed=True):\n",
    "    ''' Returns RusselRao similaritiy between 2 users. The RusselRao similarity just counts the number\n",
    "    of common non-zero components of the two vectors and divides this number by N, where N is the length\n",
    "    of the vectors. If normed=False, the division by N is omitted.\n",
    "    '''\n",
    "    # Get the list of shared_items\n",
    "    si={}\n",
    "    commons=0\n",
    "    for item in prefs[person1]: \n",
    "        if prefs[person1][item]==1 and prefs[person2][item]==1:   \n",
    "            commons+=1\n",
    "    #print commons\n",
    "    if not normed:\n",
    "        return commons\n",
    "    else:\n",
    "        return commons*1.0/len(prefs[person1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:**\n",
    "1. Geben Sie die euklidische √Ñhnlichkeit und die Pearson √Ñhnlichkeit zwischen den Personen _Toby_ und _Lisa Rose_ aus.\n",
    "2. Diskutieren Sie die unterschiedlichen √Ñhnlichkeitswert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euklidische √Ñhnlichkeit zwischen Toby und Lisa:  0.615911621788925\n",
      "Pearson √Ñhnlichkeit zwischen Toby und Lisa:  0.9912407071619302\n"
     ]
    }
   ],
   "source": [
    "print(\"Euklidische √Ñhnlichkeit zwischen Toby und Lisa: \", sim_euclid(critics,\"Toby\",\"Lisa Rose\"))\n",
    "print(\"Pearson √Ñhnlichkeit zwischen Toby und Lisa: \", sim_pearson(critics,\"Toby\",\"Lisa Rose\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Unterschied liegt vermutlich darin, dass die Korrelation zwischen den jeweiligen Werten hoch ist. Der Abstand der Vektoren aber gro√ü. Wenn Toby und Lisa den Film beide geschaut haben, haben sie ihn meist √§hnlich Bewertet. Da Lisa aber mehr Filme geschaut hat, ist ihr Vektor weiter von Tobys entfernt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aufgabe:__\n",
    "0. Schreiben Sie eine Funktion `topMatches(prefs,person,similarity)`, welche f√ºr eine beliebige in _critics_ enthaltene Person die √Ñhnlichkeitswerte zu allen anderen Personen berechnet und in einer geordneten Liste zur√ºck gibt. Der Funktion soll als √úbergabeparameter auch die anzuwendende √Ñhnlichkeitsfunktion (`sim_euclid` oder `sim_pearson`) √ºbergeben werden k√∂nnen. Berechnen Sie mit dieser Funktion f√ºr jede Person die *top matches*, zun√§chst unter Verwendung der euklidischen- dann unter Verwendung der Pearson-√Ñhnlichkeit.\n",
    "1. Geben Sie mit der implementierten Funktion die *top matches* der Person Toby aus.\n",
    "2. Vergleichen Sie die beiden √Ñhnlichkeitsma√üe. Welches √Ñhnlichkeitsma√ü erscheint Ihnen f√ºr diesen Anwendungsfall sinnvoller und warum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisa Rose's euclid Matches:  {'Mick LaSalle': 0.8092564301694538, 'Michael Phillips': 0.7815501047457912, 'Claudia Puig': 0.7597469266479578, 'Jack Matthews': 0.7208254886814803, 'Gene Seymour': 0.714462989423532, 'Toby': 0.615911621788925}\n",
      "\n",
      "Lisa Rose's pearson Matches:  {'Toby': 0.9912407071619302, 'Jack Matthews': 0.747017880833996, 'Mick LaSalle': 0.5940885257860046, 'Claudia Puig': 0.5669467095138409, 'Michael Phillips': 0.40451991747794525, 'Gene Seymour': 0.39605901719066977}\n",
      "\n",
      "Gene Seymour's euclid Matches:  {'Jack Matthews': 0.9090909090909091, 'Lisa Rose': 0.714462989423532, 'Mick LaSalle': 0.6978305207480379, 'Michael Phillips': 0.6737986373538911, 'Claudia Puig': 0.6622946603252993, 'Toby': 0.5108747069239427}\n",
      "\n",
      "Gene Seymour's pearson Matches:  {'Jack Matthews': 0.963795681875633, 'Mick LaSalle': 0.4117647058823529, 'Lisa Rose': 0.39605901719066977, 'Toby': 0.3812464258315117, 'Claudia Puig': 0.31497039417435596, 'Michael Phillips': 0.20459830184114203}\n",
      "\n",
      "Michael Phillips's euclid Matches:  {'Lisa Rose': 0.7815501047457912, 'Claudia Puig': 0.7759907622602041, 'Mick LaSalle': 0.7166996059009808, 'Gene Seymour': 0.6737986373538911, 'Jack Matthews': 0.6534537935444722, 'Toby': 0.5584815598877471}\n",
      "\n",
      "Michael Phillips's pearson Matches:  {'Claudia Puig': 1.0000000000000002, 'Lisa Rose': 0.40451991747794525, 'Gene Seymour': 0.20459830184114203, 'Jack Matthews': 0.13483997249264842, 'Mick LaSalle': -0.2581988897471611, 'Toby': -1.0}\n",
      "\n",
      "Claudia Puig's euclid Matches:  {'Michael Phillips': 0.7759907622602041, 'Lisa Rose': 0.7597469266479578, 'Mick LaSalle': 0.6964322291925094, 'Gene Seymour': 0.6622946603252993, 'Jack Matthews': 0.6534537935444722, 'Toby': 0.6246387977050463}\n",
      "\n",
      "Claudia Puig's pearson Matches:  {'Michael Phillips': 1.0000000000000002, 'Toby': 0.8934051474415644, 'Lisa Rose': 0.5669467095138409, 'Mick LaSalle': 0.5669467095138409, 'Gene Seymour': 0.31497039417435596, 'Jack Matthews': 0.02857142857142857}\n",
      "\n",
      "Mick LaSalle's euclid Matches:  {'Lisa Rose': 0.8092564301694538, 'Michael Phillips': 0.7166996059009808, 'Gene Seymour': 0.6978305207480379, 'Claudia Puig': 0.6964322291925094, 'Jack Matthews': 0.6666666666666666, 'Toby': 0.6666666666666666}\n",
      "\n",
      "Mick LaSalle's pearson Matches:  {'Toby': 0.9244734516419051, 'Lisa Rose': 0.5940885257860046, 'Claudia Puig': 0.5669467095138409, 'Gene Seymour': 0.4117647058823529, 'Jack Matthews': 0.21128856368212914, 'Michael Phillips': -0.2581988897471611}\n",
      "\n",
      "Jack Matthews's euclid Matches:  {'Gene Seymour': 0.9090909090909091, 'Lisa Rose': 0.7208254886814803, 'Mick LaSalle': 0.6666666666666666, 'Michael Phillips': 0.6534537935444722, 'Claudia Puig': 0.6534537935444722, 'Toby': 0.5227744249483389}\n",
      "\n",
      "Jack Matthews's pearson Matches:  {'Gene Seymour': 0.9637956818756331, 'Lisa Rose': 0.7470178808339961, 'Toby': 0.6628489803598703, 'Mick LaSalle': 0.21128856368212914, 'Michael Phillips': 0.13483997249264842, 'Claudia Puig': 0.02857142857142857}\n",
      "\n",
      "Toby's euclid Matches:  {'Mick LaSalle': 0.6666666666666666, 'Claudia Puig': 0.6246387977050463, 'Lisa Rose': 0.615911621788925, 'Michael Phillips': 0.5584815598877471, 'Jack Matthews': 0.5227744249483389, 'Gene Seymour': 0.5108747069239427}\n",
      "\n",
      "Toby's pearson Matches:  {'Lisa Rose': 0.9912407071619302, 'Mick LaSalle': 0.9244734516419051, 'Claudia Puig': 0.8934051474415642, 'Jack Matthews': 0.6628489803598703, 'Gene Seymour': 0.3812464258315117, 'Michael Phillips': -1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def topMatches(prefs,person,similarity):\n",
    "    similarities = {}\n",
    "    for personB in prefs:\n",
    "        if personB != person:\n",
    "            similarities[personB] = similarity(prefs,person,personB)\n",
    "    similarities = dict(sorted(similarities.items(), key=lambda x:x[1],reverse=True))\n",
    "    return similarities\n",
    " \n",
    "for person in critics:\n",
    "    print(f\"{person}'s euclid Matches: \", topMatches(critics,person,sim_euclid))\n",
    "    print(\"\")\n",
    "    print(f\"{person}'s pearson Matches: \", topMatches(critics,person,sim_pearson))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toby's euklidische TopMatches:  {'Mick LaSalle': 0.6666666666666666, 'Claudia Puig': 0.6246387977050463, 'Lisa Rose': 0.615911621788925, 'Michael Phillips': 0.5584815598877471, 'Jack Matthews': 0.5227744249483389, 'Gene Seymour': 0.5108747069239427}\n",
      "\n",
      "Toby's pearson TopMatches:  {'Lisa Rose': 0.9912407071619302, 'Mick LaSalle': 0.9244734516419051, 'Claudia Puig': 0.8934051474415642, 'Jack Matthews': 0.6628489803598703, 'Gene Seymour': 0.3812464258315117, 'Michael Phillips': -1.0}\n"
     ]
    }
   ],
   "source": [
    "    print(\"Toby's euklidische TopMatches: \", topMatches(critics,\"Toby\",sim_euclid))\n",
    "    print(\"\")\n",
    "    print(\"Toby's pearson TopMatches: \", topMatches(critics,\"Toby\",sim_pearson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn man sich die Werte anschaut, erkennt man, dass der euklidische Match alle User in etwa √§hnlich passend bewertet. Das kann an dem zu kleinen Datensatz liegen. F√ºr unsere Zwecke ist das also eher ungenau und nicht aussagekr√§ftig genug. \n",
    "Das Pearson √Ñhnlichkeitsma√ü scheint uns f√ºr diesen Fall ein wenig passender, da die Unterscheidung deutlicher ist. Der Wert -1.0 bei der Pearson√§hnlichkeit bedeutet eine Korrelation bei der die Interessen gegenl√§ufig sind. Das finden wir bei der eukliden √Ñhnlichkeit nicht. Hier hat Michael einen positiven Wert und ist z.B. nicht gegenl√§ufig zu Toby. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung von Empfehlungen mit User basiertem Collaborative Filtering\n",
    "F√ºr die Produkte, die von einer Person noch nicht gekauft wurden, sollen Empfehlungen berechnet werden. Die Empfehlungen k√∂nnen ebenfalls Werte zwischen 1 (wird nicht empfohlen) und 5 (wird stark empfohlen) annehmen. F√ºr die Berechnung der Empfehlung werden die Bewertungen des jeweiligen Produkts durch die anderen Personen herangezogen. Dabei werden die Bewertungen der √§hnlichen Personen (d.h. hoher Pearson-Korrelationswert) st√§rker mit einbezogen als die Bewertungen durch Personen mit einem niedrigen Korrelationswert.\n",
    "\n",
    "__Beispiel:__\n",
    "Toby hat die Filme *The Night Listener*, _Lady in the Water_ und _Just My Luck_ noch nicht gekauft. F√ºr diese Filme soll f√ºr Toby eine Empfehlung berechnet werden.\n",
    "In der unten aufgef√ºhrten Tabelle enth√§lt die zweite Spalte die _Pearson-√Ñhnlichkeitswerte_ zwischen Toby und den anderen Personen. Die Spalten 3, 5 und 7 enthalten die Bewertungen der Filme *The Night Listener*, _Lady in the Water_ und _Just My Luck_ durch die anderen Personen. Die Spalten 4, 6 und 8 enthalten die jeweilige Filmbewertung gewichtet (mulipliziert) mit den √Ñhnlichkeitswerten der jeweiligen Person. Es f√§llt auf, dass in der Tabelle _Michael_ nicht enthalten ist. Das liegt daran, dass _Michael_ und _Toby_ einen negativen √Ñhnlichkeitswert aufweisen, d.h. deren Interessen sind gegenl√§ufig. Personen mit negativem √Ñhnlichkeitswert sollten f√ºr Empfehlungen nicht ber√ºcksichtigt werden.\n",
    "Die Zeile _Sum_ enth√§lt die Summe aller gewichteten Bewertungen. Aus diesem Wert allein kann die Empfehlung noch nicht abgeleitet werden, da Filme die nur von wenigen Personen bewertet wurden, eine relativ kleine Summe ergeben. Deshalb sollte _Sum_ noch durch die Anzahl der Bewertungen f√ºr diesen Film geteilt werden. Oder besser: Nicht durch die Summe der Bewertungen, sondern durch die Summe der relevanten √Ñhnlichkeitswerte (*KSum*). Der resultierende Empfehlungswert ist in der letzten Zeile eingetragen.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/recommenderFilmCalculation.PNG\" style=\"width:500px\" align=\"center\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aufgabe:__\n",
    "Schreiben Sie eine Funktion *getRecommendations(prefs,person,similarity)*, mit der die Empfehlungswerte berechnet werden k√∂nnen und bestimmen Sie die Empfehlungswerte f√ºr Toby. Der Funktion wird  \n",
    "\n",
    "* das Dictionary _critics_ mit den Filmbewertungen, \n",
    "* der Name der Person, f√ºr welche Empfehlungen berechnet werden sollen\n",
    "* die Methode f√ºr die Berechnung der √Ñhnlichkeit *sim_euclid* oder *sim_pearson*\n",
    "\n",
    "√ºbergeben. Die Methode soll eine geordnete Liste zur√ºck geben. Jedes Listenelement enth√§lt an erster Stelle den berechneten Empfehlungswert und an zweiter Stelle den Namen des Films. Die Liste soll nach Empfehlungswerten absteigend geordnet sein.\n",
    "\n",
    "Testen Sie diese Funktion indem Sie die Empfehlungen f√ºr _Toby_ berechnen und mit den Werten in der oben aufgef√ºhrten Tabelle vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5309807037655645, 'Just My Luck'], [2.8325499182641622, 'Lady in the Water'], [3.3477895267131013, 'The Night Listener']]\n"
     ]
    }
   ],
   "source": [
    "uniqueFilms = set()\n",
    "# get list of all films in dictionary\n",
    "for person in critics:\n",
    "    uniqueFilms.update(critics[person])\n",
    "# function to get reccomendation user based\n",
    "def getRecommendations(pref, person, similarity):\n",
    "    recommendations = []\n",
    "    # get the most similar person\n",
    "    similarities = topMatches(pref,person,similarity)\n",
    "    # get films that were not watched by person\n",
    "    notYetWatched = uniqueFilms.copy()\n",
    "    for film in pref[person]:\n",
    "        if film in uniqueFilms:\n",
    "            notYetWatched.remove(film)\n",
    "    # calculate recommendation per film\n",
    "    for film in notYetWatched:\n",
    "        sumOfRatings = 0\n",
    "        kSum = 0\n",
    "        for personP in similarities:\n",
    "            #ignore person with negative similarity or without a rating for that film\n",
    "            prefPerson = pref.get(personP)\n",
    "            rating = prefPerson.get(film)\n",
    "            if(similarities[personP] < 0 or rating is None):\n",
    "                continue\n",
    "            kSum = kSum + similarities[personP]\n",
    "            sumOfRatings = sumOfRatings + similarities[personP]*rating\n",
    "        recommendations.append([sumOfRatings/kSum,film])\n",
    "    return recommendations\n",
    "\n",
    "    \n",
    "recommendations = getRecommendations(critics, \"Toby\", sim_pearson)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich\n",
    "Vegleicht man unser Ergebnis mit der Tabelle, stimmt es √ºberein, wenn man die Werte runden w√ºrde. Da gerundete Werte f√ºr die eventuelle Weiterverarbeitung ungenaue Vorhersagen bedeuten k√∂nnten, lassen wir sie aber ungerundet stehen. \"The Night Listener\" sollte Toby demnach am ehesten empfohlen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung von Empfehlungen mit Item basiertem Collaborative Filtering\n",
    "In den vorigen Aufgaben wurden √Ñhnlichkeiten zwischen Personen bestimmt und f√ºr Produktempfehlungen benutzt (User basiertes Collaborative Filtering). Jetzt soll die √Ñhnlichkeit zwischen Produkten berechnet werden und auf der Basis dieser Produkt√§hnlichkeit Empfehlungen berechnet werden (Item basiertes Collaborative Filtering).\n",
    "\n",
    "Dabei sollen die bereits implementierten √Ñhnlichkeitsfunktion *sim_euclid* und *sim_pearson* sowie die √Ñhnlichkeeits-Sortierfunktion *topMatches* unver√§ndert eingesetzt werden.\n",
    "\n",
    "__Aufgabe:__\n",
    "\n",
    "1. Implementieren Sie eine Funktion, welche das Bewertungsdictionary *critics* derart transformiert, dass die Funktionen `sim_euclid`, `sim_pearson` und `topMatches` f√ºr das Item-basierte CF unver√§ndert eingesetzt werden k√∂nnen. Die transformierte Matrix soll unter dem Namen *transCritics* abgespeichert werden.\n",
    "2. Schreiben Sie eine Funktion `calculateSimilarItems`, die aus der transformierten Matrix *transCritics* ein Dictionary berechnet, welches die √Ñhnlichkeit zwischen allen Filmen beschreibt. Die Keys des Dictionary sind die Filmnamen. Die Values sind geordnete Listen, welche die Funktion `topMatches` zur√ºckgibt, wenn sie f√ºr die Filme (nicht f√ºr die User) aufgerufen wird. Dieses Dictionary wird an das aufrufende Programm zur√ºck geben. \n",
    "3. Schreiben Sie eine Funktion `getRecommendedItems`, welche basierend auf dem im unten aufgef√ºhrten Beispiel dargestellten Verfahren unter Vorgabe der Bewertungsmatrix und der zu verwendenden √Ñhnlichkeitsfunktion Produktempfehlungen berechnet.\n",
    "4. Testen Sie die Funktion indem Sie die Empfehlungen f√ºr Toby berechnen und mit den Werten in der unten aufgef√ºhrten Tabelle vergleichen\n",
    "\n",
    "__Erl√§uterndes Beispiel:__\n",
    "\n",
    "_Toby_ hat die Filme *The Night Listener*, *Lady in the Water* und *Just My Luck* noch nicht gekauft. F√ºr diese Filme soll f√ºr *Toby* eine Empfehlung berechnet werden. Gekauft und bewertet hat *Toby* die Filme *Snakes on a plane*, *Superman Returns* und *You and me and Dupree*. Diese bereits vorhandenen Filme bilden die erste Spalte der unten dargestellten Matrix. In der zweiten Spalte befinden sich _Toby's_ Bewertungen dieser Filme. Die Spalten 3,5 und 7 enthalten die √Ñhnlichkeitswerte (mit *calculateSimilarItems* unter Verwendung des normierten euklidischen √Ñhnlichkeitsma√ües berechnet) zwischen den drei von *Toby* noch nicht gekauften Filmen und den drei von _Toby_ bewerteten Filmen. Diese √Ñhnlichkeitswerte werden jeweils mit _Toby's_ Bewertungen multipliziert. Das Resultat dieser Multiplikation befindet sich in den Spalten 4,6 und 8. Der finale Empfehlungswert f√ºr die von _Toby_ noch nicht gekauften Filme wird berechnet in dem in den Spalten 4,6 und 8 zun√§chst die Summe √ºber die Werte dieser Spalte in den drei oberen Zeilen berechnet wird und durch die Summe √ºber die Werte der Spalten 3,5 und 7 geteilt wird. Im Fall, dass die *Pearson-Korrelation* zwischen den Filmen als √Ñhnlichkeitswert herangezogen wird, k√∂nnen negative √Ñhnlichkeitswerte auftreten. Dann soll in die Berechnung eines Empfehlungswert f√ºr Film A nur dann die Bewertung von Film B einflie√üen, wenn der Korrelationswert zwischen beiden $>0$ ist.  \n",
    "\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/recommenderFilmItemBased.PNG\" style=\"width:500px\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'You, Me and Dupree': {'Lisa Rose': 2.5, 'Gene Seymour': 3.5, 'Claudia Puig': 2.5, 'Mick LaSalle': 2.0, 'Jack Matthews': 3.5, 'Toby': 1.0}, 'Just My Luck': {'Lisa Rose': 3.0, 'Gene Seymour': 1.5, 'Claudia Puig': 3.0, 'Mick LaSalle': 2.0}, 'Superman Returns': {'Lisa Rose': 3.5, 'Gene Seymour': 5.0, 'Michael Phillips': 3.5, 'Claudia Puig': 4.0, 'Mick LaSalle': 3.0, 'Jack Matthews': 5.0, 'Toby': 4.0}, 'Lady in the Water': {'Lisa Rose': 2.5, 'Gene Seymour': 3.0, 'Michael Phillips': 2.5, 'Mick LaSalle': 3.0, 'Jack Matthews': 3.0}, 'The Night Listener': {'Lisa Rose': 3.0, 'Gene Seymour': 3.0, 'Michael Phillips': 4.0, 'Claudia Puig': 4.5, 'Mick LaSalle': 3.0, 'Jack Matthews': 3.0}, 'Snakes on a Plane': {'Lisa Rose': 3.5, 'Gene Seymour': 3.5, 'Michael Phillips': 3.0, 'Claudia Puig': 3.5, 'Mick LaSalle': 4.0, 'Jack Matthews': 4.0, 'Toby': 4.5}}\n"
     ]
    }
   ],
   "source": [
    "def transformCritics(critics):\n",
    "    transCritics = {}\n",
    "    for film in uniqueFilms:\n",
    "        transCritics[film] = {}\n",
    "    for k in transCritics:\n",
    "        for key, value in critics.items():\n",
    "            if k in value:\n",
    "                transCritics[k][key] = value[k]\n",
    "       \n",
    "    return transCritics        \n",
    "transCritics = transformCritics(critics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## last.fm Musikempfehlungen\n",
    "\n",
    "__Aufgabe:__\n",
    "\n",
    "1. Installieren Sie das Package pylast. Stellen Sie durch Aufruf der Funktion *network=pylast.LastFMNetwork()* eine Verbindung zu *last.fm* her. Beim Aufruf der Funktion wird die Anmeldung und Authentifizierung durchgef√ºhrt. Die Funktion gibt ein Objekt der Klasse *Network* zur√ºck. √úber dieses Objekt k√∂nnen Methoden, wie\n",
    "\n",
    "    * *get_artist(\"kuenstlerName\")* (liefert Objekt der Klasse *Artist*)\n",
    "    * *get_album(\"albumName\")* (liefert Objekt der Klasse *Album*)\n",
    "    * *get_track(\"songName\")* (liefert Objekt der Klasse *Track*)\n",
    "    * *get_user(\"userName\"):* (liefert Objekt der Klasse *Tag*)\n",
    "    * usw.\n",
    "    \n",
    "      aufgerufen werden. Die Menge aller verf√ºgbaren Klassen und deren Attribute und Methoden k√∂nnen dem Modul _pylast.py_ entnommen werden.\n",
    "\n",
    "1. Rufen Sie √ºber das oben instanziierte *Network*-Objekt die Methode `get_artist(\"BandIhrerWahl\")` auf.\n",
    "2. Rufen Sie √ºber das oben instanziierte *Artist*-Objekt die Methode `topfans=get_top_fans(10)` auf. Die Methode gibt eine Liste von _User_-Objekt/Gewichtung-Paaren zur√ºck. Die Gewichtungen von Objekten werden in diesem Versuch nicht ben√∂tigt. Legen Sie deshalb mit `group=[a.item for a in topfan]` eine Liste an, die nur noch die User Objekte enth√§lt. **Wichtige Anmerkung:** Seit August 2015 gibt es Probleme mit der lastFM API Methode `get_top_fans()` (siehe auch: [pylast issues](https://github.com/pylast/pylast/issues/155s)). Falls am Versuchstermin der Fehler noch nicht behoben ist, k√∂nnen Sie den unten stehenden Code benutzen. Darin wird versucht auf die API-Methode zuzugreifen. Falls das nicht m√∂glich ist, wird eine vordefinierte Liste von Usern angewandt. Diese Liste repr√§sentiert die *Top Fans* der Band *Slipknot* im Fr√ºhjahr 2015. \n",
    "3. Implementieren Sie eine Funktion `createLastfmUserDict()`. Dieser Funktion soll, die oben angelegte Liste von *User*-Objekten _group_ √ºbergeben werden. F√ºr jeden User in *group* sollen die 20 beliebtesten Bands mit der Methode `topartists=get_top_artists()[0:20]` bestimmt werden. Die Methode gibt eine Liste von *Artist*-Objekt/Gewichtung-Paaren zur√ºck. Die Gewichtungen von Objekten werden in diesem Versuch nicht ben√∂tigt. Auf das *i.te* *Artist*-Objekt selbst kann mit `topartists[i].item` zugegriffen werden. Die Menge aller Bands, die auf diese Weise gesammelt werden, wird im folgenden mit _AllBands_ bezeichnet. D.h. in *AllBands* befinden sich alle Bands, die f√ºr mindestens einen User in *group* zu den Top-20 geh√∂ren. Nun soll ein verschachteltes Dictionary mit Namen *userDict* wie folgt angelegt werden:\n",
    "\n",
    "    * Die Keys sind die Namen der _User_-Objekte in _group_. Auf den Namen eines Objekts kann mit `get_name()` zugegriffen werden.\n",
    "    * Die Values sind selbst wieder Dictionaries, deren Keys die Namen der Bands in *AllBands* sind. Achten Sie auch hier darauf, dass Sie nicht das *Artist*-Objekt selbst, sondern dessen Namen als Key verwenden. \n",
    "    * F√ºr den User *a* und die Band *b* ist der Value `userDict[a][b]= 1`, falls *b* zu den Top-20 des Users *a* geh√∂rt. Andernfalls ist `userDict[a][b]= 0`. \n",
    "    \n",
    "    Das derart angelegte Dictionary soll von der Funktion zur√ºckgegeben werden. \n",
    "4. W√§hlen Sie jetzt einen beliebigen User aus *group*. Bestimmen Sie zu diesem User die √§hnlichsten User in *group* durch Anwendung der im ersten Teilversuch implementierten Funktion `topMatches()`. Der Funktion wird das angelegte *userDict* und der Name des gew√§hlten Users √ºbergeben. Als √Ñhnlichkeitsma√ü soll die euklidische Metrik angewandt werden.\n",
    "5. Bestimmen Sie dann f√ºr den gew√§hlten User Band-Empfehlungen durch Anwendung der im ersten Teilversuch implementierten Funktion `getRecommendations()`. Der Funktion wird das angelegte *userDict* und der Name des gew√§hlten Users √ºbergeben. Als √Ñhnlichkeitsma√ü soll die euklidische Metrik, danach die Russel_Rao Metrik, angewandt werden.     \n",
    "6. Diskutieren Sie das Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylast\n",
    "nw=pylast.LastFMNetwork(api_key = \"993a5bd9d79a98a53677570368d55acd\",api_secret = \"9b8de0b57903ac007cdd8ec9003b341e\",username = \"pythonlab\")\n",
    "band='Slipknot'\n",
    "art1 = nw.get_artist(band)\n",
    "print(\"Most similar (as calculated by lastFM) for artist: \",band)\n",
    "for it in art1.get_similar(5):\n",
    "    print(\"%3.3f \\t %s\"%(it.match, it.item))\n",
    "try:\n",
    "    topfan = art1.get_top_fans()\n",
    "    group = [a.item for a in topfan]\n",
    "except:\n",
    "    print(\"\\nlastFM API Error for method get_top_fans\\nApply predefined group of users\")\n",
    "    usernames=['BrunoJoS','DPREBOYE','MPistol40','NemoNightfall','SkyRif','Wags1382','Znapsen','cortapsyco','emill_67','sattuviitana']\n",
    "    group=[]\n",
    "    for u in usernames:\n",
    "        u1 = nw.get_user(u)\n",
    "        group.append(u1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "664px",
   "left": "0px",
   "right": "1209.67px",
   "top": "125.333px",
   "width": "212px"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
